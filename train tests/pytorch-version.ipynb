{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2497756,"sourceType":"datasetVersion","datasetId":1512337}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import glob\nimport cv2\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import models, transforms\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom scipy.optimize import minimize","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T20:39:48.876609Z","iopub.execute_input":"2025-05-20T20:39:48.876780Z","iopub.status.idle":"2025-05-20T20:39:56.793838Z","shell.execute_reply.started":"2025-05-20T20:39:48.876763Z","shell.execute_reply":"2025-05-20T20:39:56.793007Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Custom Dataset\nclass HandGestureDataset(Dataset):\n    def __init__(self, images, labels, transform=None):\n        self.images = images\n        self.labels = labels\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self, idx):\n        image = self.images[idx]\n        label = self.labels[idx]\n        \n        # Convert to PyTorch expected format (C, H, W)\n        image = np.transpose(image, (2, 0, 1)).astype(np.float32)\n        \n        if self.transform:\n            image = torch.from_numpy(image)\n            image = self.transform(image)\n        else:\n            image = torch.from_numpy(image)\n            \n        return image, label\n\n# Data augmentation\ntrain_transform = transforms.Compose([\n    transforms.RandomRotation(15),\n    transforms.RandomHorizontalFlip(),\n    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T20:41:36.814161Z","iopub.execute_input":"2025-05-20T20:41:36.814775Z","iopub.status.idle":"2025-05-20T20:41:36.821121Z","shell.execute_reply.started":"2025-05-20T20:41:36.814749Z","shell.execute_reply":"2025-05-20T20:41:36.820398Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Model Builder\nclass HandGestureModel(nn.Module):\n    def __init__(self, base_model_name, num_classes=14):\n        super(HandGestureModel, self).__init__()\n        \n        # Load pre-trained model\n        if base_model_name == 'mobilenet_v2':\n            self.base_model = models.mobilenet_v2(pretrained=True)\n            num_ftrs = self.base_model.classifier[1].in_features\n            self.base_model.classifier = nn.Identity()\n        elif base_model_name == 'vgg19':\n            self.base_model = models.vgg19(pretrained=True)\n            num_ftrs = self.base_model.classifier[0].in_features\n            self.base_model.classifier = nn.Identity()\n        else:\n            raise ValueError(f\"Unsupported model: {base_model_name}\")\n        \n        # Freeze base model layers\n        for param in self.base_model.parameters():\n            param.requires_grad = False\n            \n        # Custom classifier\n        self.classifier = nn.Sequential(\n            nn.BatchNorm1d(num_ftrs),\n            nn.Dropout(0.3),\n            nn.Linear(num_ftrs, 256),\n            nn.ReLU(),\n            nn.BatchNorm1d(256),\n            nn.Dropout(0.3),\n            nn.Linear(256, num_classes)\n        )\n        \n    def forward(self, x):\n        x = self.base_model(x)\n        # Apply global average pooling if needed\n        if len(x.shape) > 2:\n            x = torch.mean(x, dim=[2, 3])\n        x = self.classifier(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T20:42:04.360651Z","iopub.execute_input":"2025-05-20T20:42:04.361442Z","iopub.status.idle":"2025-05-20T20:42:04.367808Z","shell.execute_reply.started":"2025-05-20T20:42:04.361416Z","shell.execute_reply":"2025-05-20T20:42:04.366998Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Training function\ndef train_model(model, dataloaders, criterion, optimizer, num_epochs=50, patience=5):\n    best_model_wts = model.state_dict()\n    best_acc = 0.0\n    no_improve_epochs = 0\n    \n    for epoch in range(num_epochs):\n        print(f'Epoch {epoch+1}/{num_epochs}')\n        print('-' * 10)\n        \n        # Each epoch has a training and validation phase\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()\n            else:\n                model.eval()\n                \n            running_loss = 0.0\n            running_corrects = 0\n            \n            # Iterate over data\n            for inputs, labels in dataloaders[phase]:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n                \n                # Zero the parameter gradients\n                optimizer.zero_grad()\n                \n                # Forward\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    _, preds = torch.max(outputs, 1)\n                    loss = criterion(outputs, labels)\n                    \n                    # Backward + optimize only if in training phase\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n                        \n                # Statistics\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n            \n            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n            \n            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n            \n            # Deep copy the model if best validation accuracy\n            if phase == 'val' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = model.state_dict().copy()\n                no_improve_epochs = 0\n            elif phase == 'val':\n                no_improve_epochs += 1\n        \n        # Early stopping\n        if no_improve_epochs >= patience:\n            print(f'Early stopping at epoch {epoch+1}')\n            break\n                \n        print()\n    \n    print(f'Best val Acc: {best_acc:.4f}')\n    \n    # Load best model weights\n    model.load_state_dict(best_model_wts)\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T20:45:11.942452Z","iopub.execute_input":"2025-05-20T20:45:11.943008Z","iopub.status.idle":"2025-05-20T20:45:11.952219Z","shell.execute_reply.started":"2025-05-20T20:45:11.942976Z","shell.execute_reply":"2025-05-20T20:45:11.951629Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Function to get predictions\ndef get_predictions(model, dataloader):\n    model.eval()\n    all_preds = []\n    \n    with torch.no_grad():\n        for inputs, _ in dataloader:\n            inputs = inputs.to(device)\n            outputs = model(inputs)\n            probs = nn.Softmax(dim=1)(outputs)\n            all_preds.append(probs.cpu().numpy())\n            \n    return np.vstack(all_preds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T20:45:49.121891Z","iopub.execute_input":"2025-05-20T20:45:49.122580Z","iopub.status.idle":"2025-05-20T20:45:49.126779Z","shell.execute_reply.started":"2025-05-20T20:45:49.122558Z","shell.execute_reply":"2025-05-20T20:45:49.126037Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Main execution\nif __name__ == \"__main__\":\n    # Load all image paths\n    all_paths = glob.glob('/kaggle/input/hg14-handgesture14-dataset/HG14/HG14-Hand Gesture/*/*.jpg')\n    \n    # Prepare dataset\n    data, labels = [], []\n    for path in tqdm(all_paths):\n        img = cv2.imread(path)\n        img = cv2.resize(img, (128, 128))\n        label = path.split('/')[-2]\n        data.append(img)\n        labels.append(label)\n    \n    data = np.array(data) / 255.0\n    labels_factorized = pd.factorize(labels)[0]\n    \n    # Split data\n    X_temp, X_test, y_temp, y_test = train_test_split(\n        data, labels_factorized, test_size=0.10, stratify=labels_factorized, random_state=42\n    )\n    X_train, X_val, y_train, y_val = train_test_split(\n        X_temp, y_temp, test_size=0.20, stratify=y_temp, random_state=42\n    )\n    \n    # Create datasets\n    train_dataset = HandGestureDataset(X_train, y_train, transform=train_transform)\n    val_dataset = HandGestureDataset(X_val, y_val)\n    test_dataset = HandGestureDataset(X_test, y_test)\n    \n    # Create dataloaders\n    batch_size = 20\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n    \n    dataloaders = {\n        'train': train_loader,\n        'val': val_loader\n    }\n    \n    # Settings\n    models_dict = {\n        'mobilenet_v2': 'mobilenet_v2',\n        'vgg19': 'vgg19'\n    }\n    \n    trained_models = {}\n    val_preds = {}\n    test_preds = {}\n    \n    # Train models\n    for name, model_type in models_dict.items():\n        print(f\"\\nTraining {name}\")\n        model = HandGestureModel(model_type).to(device)\n        criterion = nn.CrossEntropyLoss()\n        optimizer = optim.Adam(model.parameters())\n        \n        # Train and validate\n        model = train_model(model, dataloaders, criterion, optimizer)\n\n        \n        # Save trained model\n        trained_models[name] = model\n        \n        #saving\n        torch.save(model.state_dict(), f\"{name}_hand_gesture_model.pth\")\n        \n        # Get predictions\n        val_preds[name] = get_predictions(model, val_loader)\n        test_preds[name] = get_predictions(model, test_loader)\n    \n    # Ensemble with Dirichlet optimization\n    val_stack = np.stack([val_preds[name] for name in models_dict.keys()], axis=-1)\n    \n    def dirichlet_loss(weights):\n        ensemble_pred = np.tensordot(val_stack, weights, axes=([3], [0]))\n        y_val_onehot = np.zeros((len(y_val), 14))\n        y_val_onehot[np.arange(len(y_val)), y_val] = 1\n        return -np.mean(np.sum(y_val_onehot * np.log(ensemble_pred + 1e-8), axis=1))\n    \n    init_weights = np.ones(len(models_dict)) / len(models_dict)\n    bounds = [(0, 1)] * len(models_dict)\n    constraints = [{'type': 'eq', 'fun': lambda w: 1 - sum(w)}]\n    \n    res = minimize(dirichlet_loss, init_weights, bounds=bounds, constraints=constraints)\n    final_weights = res.x\n    print(\"Optimized Weights:\", final_weights)\n    np.save('models/ensemble_weights.npy', final_weights)\n    \n    # Final Test Prediction\n    test_stack = np.stack([test_preds[name] for name in models_dict.keys()], axis=-1)\n    ensemble_test_pred = np.tensordot(test_stack, final_weights, axes=([3], [0]))\n    ensemble_test_labels = np.argmax(ensemble_test_pred, axis=1)\n    \n    # Extract ground truth labels from test dataset\n    test_labels = []\n    for _, label in test_loader:\n        test_labels.extend(label.numpy())\n    test_labels = np.array(test_labels)\n    \n    print(\"Ensemble Accuracy on Test Set:\", accuracy_score(test_labels, ensemble_test_labels))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T20:46:09.446064Z","iopub.execute_input":"2025-05-20T20:46:09.446617Z","iopub.status.idle":"2025-05-20T21:13:38.837734Z","shell.execute_reply.started":"2025-05-20T20:46:09.446595Z","shell.execute_reply":"2025-05-20T21:13:38.836604Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 14000/14000 [01:48<00:00, 129.56it/s]\n/tmp/ipykernel_35/540360274.py:16: FutureWarning: factorize with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n  labels_factorized = pd.factorize(labels)[0]\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n","output_type":"stream"},{"name":"stdout","text":"\nTraining mobilenet_v2\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 13.6M/13.6M [00:00<00:00, 113MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/50\n----------\ntrain Loss: 1.4239 Acc: 0.5115\nval Loss: 1.1408 Acc: 0.6087\n\nEpoch 2/50\n----------\ntrain Loss: 1.1665 Acc: 0.5847\nval Loss: 1.0882 Acc: 0.6115\n\nEpoch 3/50\n----------\ntrain Loss: 1.1208 Acc: 0.6003\nval Loss: 0.9427 Acc: 0.6690\n\nEpoch 4/50\n----------\ntrain Loss: 1.0616 Acc: 0.6265\nval Loss: 0.9882 Acc: 0.6579\n\nEpoch 5/50\n----------\ntrain Loss: 1.0782 Acc: 0.6182\nval Loss: 0.9313 Acc: 0.6778\n\nEpoch 6/50\n----------\ntrain Loss: 1.0330 Acc: 0.6323\nval Loss: 0.9532 Acc: 0.6714\n\nEpoch 7/50\n----------\ntrain Loss: 1.0218 Acc: 0.6379\nval Loss: 0.8478 Acc: 0.7040\n\nEpoch 8/50\n----------\ntrain Loss: 1.0289 Acc: 0.6332\nval Loss: 0.9056 Acc: 0.6921\n\nEpoch 9/50\n----------\ntrain Loss: 0.9988 Acc: 0.6434\nval Loss: 0.9117 Acc: 0.6762\n\nEpoch 10/50\n----------\ntrain Loss: 0.9900 Acc: 0.6532\nval Loss: 0.8963 Acc: 0.6786\n\nEpoch 11/50\n----------\ntrain Loss: 0.9759 Acc: 0.6510\nval Loss: 0.8635 Acc: 0.6988\n\nEpoch 12/50\n----------\ntrain Loss: 0.9661 Acc: 0.6546\nval Loss: 0.9184 Acc: 0.6718\nEarly stopping at epoch 12\nBest val Acc: 0.7040\n\nTraining vgg19\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n100%|██████████| 548M/548M [00:02<00:00, 209MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/50\n----------\ntrain Loss: 1.0003 Acc: 0.6591\nval Loss: 0.5931 Acc: 0.8242\n\nEpoch 2/50\n----------\ntrain Loss: 0.7070 Acc: 0.7520\nval Loss: 0.5467 Acc: 0.8353\n\nEpoch 3/50\n----------\ntrain Loss: 0.6492 Acc: 0.7673\nval Loss: 0.5349 Acc: 0.8389\n\nEpoch 4/50\n----------\ntrain Loss: 0.5988 Acc: 0.7880\nval Loss: 0.5309 Acc: 0.8516\n\nEpoch 5/50\n----------\ntrain Loss: 0.5786 Acc: 0.7936\nval Loss: 0.4891 Acc: 0.8587\n\nEpoch 6/50\n----------\ntrain Loss: 0.5697 Acc: 0.7933\nval Loss: 0.4679 Acc: 0.8659\n\nEpoch 7/50\n----------\ntrain Loss: 0.5323 Acc: 0.8084\nval Loss: 0.4411 Acc: 0.8778\n\nEpoch 8/50\n----------\ntrain Loss: 0.5241 Acc: 0.8132\nval Loss: 0.5092 Acc: 0.8659\n\nEpoch 9/50\n----------\ntrain Loss: 0.5053 Acc: 0.8183\nval Loss: 0.4399 Acc: 0.8813\n\nEpoch 10/50\n----------\ntrain Loss: 0.4924 Acc: 0.8261\nval Loss: 0.5030 Acc: 0.8679\n\nEpoch 11/50\n----------\ntrain Loss: 0.4859 Acc: 0.8289\nval Loss: 0.5375 Acc: 0.8619\n\nEpoch 12/50\n----------\ntrain Loss: 0.4547 Acc: 0.8347\nval Loss: 0.4327 Acc: 0.8786\n\nEpoch 13/50\n----------\ntrain Loss: 0.4807 Acc: 0.8297\nval Loss: 0.4089 Acc: 0.8909\n\nEpoch 14/50\n----------\ntrain Loss: 0.4724 Acc: 0.8312\nval Loss: 0.4018 Acc: 0.8885\n\nEpoch 15/50\n----------\ntrain Loss: 0.4576 Acc: 0.8413\nval Loss: 0.4235 Acc: 0.8944\n\nEpoch 16/50\n----------\ntrain Loss: 0.4423 Acc: 0.8371\nval Loss: 0.4380 Acc: 0.8913\n\nEpoch 17/50\n----------\ntrain Loss: 0.4473 Acc: 0.8432\nval Loss: 0.4059 Acc: 0.8921\n\nEpoch 18/50\n----------\ntrain Loss: 0.4317 Acc: 0.8483\nval Loss: 0.4493 Acc: 0.8905\n\nEpoch 19/50\n----------\ntrain Loss: 0.4300 Acc: 0.8459\nval Loss: 0.4174 Acc: 0.8909\n\nEpoch 20/50\n----------\ntrain Loss: 0.4190 Acc: 0.8548\nval Loss: 0.4447 Acc: 0.8925\nEarly stopping at epoch 20\nBest val Acc: 0.8944\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/540360274.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mconstraints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'type'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'eq'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fun'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirichlet_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstraints\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m     \u001b[0mfinal_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Optimized Weights:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    748\u001b[0m                                **options)\n\u001b[1;32m    749\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'slsqp'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m         res = _minimize_slsqp(fun, x0, args, jac, bounds,\n\u001b[0m\u001b[1;32m    751\u001b[0m                               constraints, callback=callback, **options)\n\u001b[1;32m    752\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'trust-constr'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/optimize/_slsqp_py.py\u001b[0m in \u001b[0;36m_minimize_slsqp\u001b[0;34m(func, x0, args, jac, bounds, constraints, maxiter, ftol, iprint, disp, eps, callback, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# ScalarFunction provides function and gradient evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m     sf = _prepare_scalar_function(func, x, jac=jac, args=args, epsilon=eps,\n\u001b[0m\u001b[1;32m    382\u001b[0m                                   \u001b[0mfinite_diff_rel_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfinite_diff_rel_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m                                   bounds=new_bounds)\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/optimize/_optimize.py\u001b[0m in \u001b[0;36m_prepare_scalar_function\u001b[0;34m(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[0;31m# ScalarFunction caches. Reuse of fun(x) during grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0;31m# calculation reduces overall function evaluations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m     sf = ScalarFunction(fun, x0, args, grad, hess,\n\u001b[0m\u001b[1;32m    292\u001b[0m                         finite_diff_rel_step, bounds, epsilon=epsilon)\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;31m# Initial function evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;31m# Initial gradient evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36m_update_fun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_update_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_updated\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m             \u001b[0mfx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfx\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lowest_f\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lowest_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# Overwriting results in undefined behaviour because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# fun(self.x) will change self.x, with the two no longer linked.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mfx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Make sure the function returns a true scalar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_35/540360274.py\u001b[0m in \u001b[0;36mdirichlet_loss\u001b[0;34m(weights)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdirichlet_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mensemble_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensordot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_stack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0my_val_onehot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m14\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0my_val_onehot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36mtensordot\u001b[0;34m(a, b, axes)\u001b[0m\n\u001b[1;32m   1089\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mna\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1091\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mas_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxes_a\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxes_b\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1092\u001b[0m                 \u001b[0mequal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: tuple index out of range"],"ename":"IndexError","evalue":"tuple index out of range","output_type":"error"}],"execution_count":7},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = HandGestureModel(model_type)\nmodel.load_state_dict(torch.load(\"mobilenet_v2_hand_gesture_model.pth\"))\nmodel.to(device)\nmodel.eval()","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null}]}